{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 4 - Basic machine learning with ```Pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing we need to do for this workshops is install both ```pytorch``` and ```scikit-learn```, along with some other packages we need for this week. You may have already installed some of this for previous classes (if so, some of this will be skipped).\n",
    "\n",
    "```\n",
    "pip install --upgrade pip\n",
    "pip install torch scikit-learn matplotlib pandas\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Load packages__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# system tools\n",
    "import os\n",
    "\n",
    "# pytorch\n",
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "# pandas\n",
    "import pandas as pd\n",
    "\n",
    "# scikit-learn\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "# matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# library for symbolic computations (we use it to plot functions)\n",
    "from sympy import symbols\n",
    "from sympy.plotting import plot\n",
    "\n",
    "# numpy\n",
    "import numpy as np\n",
    "\n",
    "# huggingface datasets\n",
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating a tensor__\n",
    "\n",
    "A tensor is a data structure similar to numpy arrays, it basically stores something like matrices, but in more than two dimensions. Different from numpy, tensors from Pytorch have handy characteristics that make it possible to easily compute and store gradients, which is useful for machine learning optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tensor = torch.tensor([[1., -1.], \n",
    "                         [1., -1.]])\n",
    "print(type(x_tensor))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are different ways to create tensors (from scratch, or by \"tensorifying\" other data structures, such as arrays).\n",
    "Take a look at the documentation for more details: https://pytorch.org/tutorials/beginner/introyt/tensors_deeper_tutorial.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Tensor to numpy array__\n",
    "\n",
    "Tensors can easily be transformed into numpy arrays (e.g., once you're done with training a neural network), which may be easier to use with tools from scientific computing libraries such as scikit-learn or scipy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tensor to numpy\n",
    "x_array = x_tensor.numpy()\n",
    "print(type(x_array))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__And back again__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# numpy to tensor\n",
    "x_tensor2 =torch.tensor(x_array)\n",
    "print(type(x_tensor2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for identity\n",
    "print(x_tensor2 == x_tensor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the data has not been changed by these transformation: all that has changed is the data structure that the data is stored in."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stochastic gradient descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember when we talked about loss functions and gradient descent in class? The goal there was to find the value of the parameters (our ```x```) which corresponds to the lowest possible value of the loss function (our ```y```). This can be done through a process called gradient descent, which involves:\n",
    "1. starting from a random initial value for x \n",
    "2. computing the derivative of the function (`d_x`) for that value\n",
    "3. computing a new value of x, with this formula: ```x_new = x_old - learning_rate * d_x```\n",
    "4. continuing, until we find an x which yields very low values of y.\n",
    "\n",
    "Let's try this out for a polynomial function (could be anything really, but for this one we know it has a minimum).\n",
    "\n",
    "Our function will be: `y = x**2 - x*3 + 2`. Let's see how our function looks:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = symbols('x')\n",
    "p1 = plot(x**2 - 3*x + 2, show=False, xlim=(-10,10), ylim=(-2,60))\n",
    "p1.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's begin by creating an initial value for ```x``` and defining the function ```y```.\n",
    "The goal is to find the _minimum_ value of y, i.e. in this case the turning point of the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([3.], \n",
    "                 requires_grad=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = x**2 - 3*x + 2\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output of this is the value of our function for `x=3`. To find lower values of y, let's perform gradient descent. In PyTorch, we can do so using optimizers (https://pytorch.org/docs/stable/optim.html), which implement multiple optimization algorithms including gradient descent (`SGD` stands for stochastic gradient descent)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Create SGD optimizer__\n",
    "\n",
    "In doing so, we also set a learning rate, which is one of the parameters for the optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.SGD([x],     # starting value\n",
    "                            lr=0.01) # learning rate\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Calculate the gradient__\n",
    "\n",
    "Let's compute the derivative of our function `y = f(x)` for our current value of `x`.\n",
    "\n",
    "We do so by running a _backwards pass_ which computes the gradient of the function ```y``` for a given value ```x```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x.grad) # examine the resulting gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Make a step in the right direction__\n",
    "\n",
    "Using information on the gradient, the optimizer will update the value of `x` based on the update equation (`x_new = x_old - learning_rate * d_new`).\n",
    "Let's first manually check what the new value of x should be."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "3 - 0.001 * x.grad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's do this with the optimizer, which is how you would normally do that when training a network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# step in the direction to minimize y\n",
    "optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the gradient to zero. (This is a bit weird but required)\n",
    "optimizer.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimizer has computed the new value of x. Let's check that this corresponds to a better value of `y`, compared to the previous `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x**2 - 3*x + 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It does, this is working as desired... but can we do better?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run this for 1000 steps__\n",
    "\n",
    "We can of course do better than this (we know that the minimum for y is around zero), so let's see what happens if we run these updates 1000 times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "values = [] # here we store the values of y as we compute them\n",
    "\n",
    "for i in range(1000):\n",
    "    #print(x)\n",
    "\n",
    "    # forward pass / or just calculate the outcome\n",
    "    y = x**2 - 3*x + 2\n",
    "    values.append(y.detach().numpy()) # detach removes gradient information and allows us to transform tensors into arrays\n",
    " \n",
    "    # backward pass on the thing we want to minimize\n",
    "    y.backward()\n",
    "\n",
    "    # take a step in the \"minimize direction\"\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero the gradient\n",
    "    optimizer.zero_grad()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have converged to the value of `x` which yields the minimum `y`. Now let's visualize the history of this process, seeing how the value of `y` changed at each step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(1000), values)\n",
    "plt.xlabel('step')\n",
    "plt.ylabel('value of y')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The process we have performed here is analogous to what you do when training any neural network using stochastic gradient descent, or similar optimization algorithms. In that case, the function you will be working with will be the **loss function** for your network (e.g., `cross-entropy loss`), rather than our polynomial function. \n",
    "The curve we have just plotted here will describe changes in your loss as a function of training. If training is going well, you will have a fairly smoothly descending curve. If the curve is much bumpier or not monotonically descending, it may be a symptom of your training process not being optimal (e.g., the learning rate not being good, or the loss function being ill-defined)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bonus task\n",
    "- Try and define some functions of your own and see if you can find the minimum. (There are tools online where you can check what the actual minimum is, to see if the algorithm gets it right!)\n",
    "- What happens if you change the learning rate? Try set it to unrealistically high or low values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a Neural Logistic Regression Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far we haven't actually looked at any text data, and we have looked at a toy example of optimization.\n",
    "Time to move on to a real example. In class 2, we looked at an example of text classification using term-document matrices to classify the sentiment of some sentences.\n",
    "In that case, we used the `scikit-learn` implementation of logistic regression. Here, we will solve the same problem using a neural network classifier. The advantage of this approach, is that we could in theory add a number of intermediate layers to improve the performance of our models (minding that overly complex networks may also *hinder* performance by overfitting)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the sst2 dataset\n",
    "dataset = load_dataset(\"glue\", \"sst2\")\n",
    "\n",
    "# select the train split\n",
    "data = dataset[\"train\"]\n",
    "X = data[\"sentence\"]\n",
    "y = data[\"label\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating train/test splits__\n",
    "\n",
    "A common practice when building ML/DL models is to use explicitly defined subsets of data for different tasks - [training vs testing](https://upload.wikimedia.org/wikipedia/commons/b/bb/ML_dataset_training_validation_test_sets.png), for example. This is slightly different from how we work when doing statistical modelling (in most cases).\n",
    "\n",
    "```scikit-learn``` has a simple tool that allows us to quickly split our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(data[\"sentence\"], \n",
    "                                                    data[\"label\"], \n",
    "                                                    test_size=0.2, \n",
    "                                                    random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Creating a document vectorizer__\n",
    "\n",
    "Similar to what we did in the previous class, we now transform our sentences into a matrix which has documents as rows, and words as columns, and whose values are normalized frequency counts. We do it using `TfidfVectorizer` from `sklearn` (or `CountVectorizer`). Do you remember what the difference was?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialize vectorizer__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Fit to the training data__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized training data\n",
    "X_train_vect = vectorizer.fit_transform(X_train)\n",
    "\n",
    "# vectorized test data\n",
    "X_test_vect = vectorizer.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert to tensors__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vectorized training data\n",
    "X_train_vect = torch.tensor(X_train_vect.toarray(), dtype=torch.float)\n",
    "\n",
    "# vectorized test data\n",
    "X_test_vect = torch.tensor(X_test_vect.toarray(), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Convert labels__ to tensors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# training labels\n",
    "y_train = torch.tensor(list(y_train), dtype=torch.float)\n",
    "# test labels\n",
    "y_test = torch.tensor(list(y_test), dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = y_train.view(y_train.shape[0], 1)\n",
    "y_test = y_test.view(y_test.shape[0], 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specifying model, loss, and optimizer\n",
    "Ok, so, now we have inputs and labels for our model. What we need to do is specify three things:\n",
    "- __our model__: we want to specify the architecture of our model;\n",
    "- __our loss function__: we want to specify what function we are using to optimize model weights to producing the best possible predictions. Since this is a binary classification task, we will use the cross-entropy function we have discussed in class;\n",
    "- __our optimizer__: the process by which we optimize weights. We will use Stochastic Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Initialization parameters for Logistic Regression__\n",
    "\n",
    "To define the architecture, we need to know how many inputs we have, i.e., the dimensionality of our vectors. We also need to know what the dimensionality of the output is."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_samples, n_features = X_train_vect.shape\n",
    "input_size = n_features \n",
    "output_size = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__1. Creating the model__\n",
    "\n",
    "Computing the output with a simple logistic regression model implies two things. First, we multiply each of our inputs by the corresponding weights, sum them, and add the bias. This is what a __linear__ (or __fully connected__) layer in a neural network architecture can do for us. Second, we apply a sigmoid transformation to the output.\n",
    "\n",
    "Let's start by implementing this simple model as a neural network. All we need is a `Linear` layer which takes a number of inputs corresponding to the number of dimensions of our document vectors, and outputs a single value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a weight and biases (betas and intercept) initialized 'randomly'\n",
    "model = nn.Linear(input_size, output_size)\n",
    "learning_rate = 0.01 # feel free to change this"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This comes with randomly initialized weights and biases for the linear layer. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__2-3. Defining the loss and the optimizer__\n",
    "\n",
    "In the first part of our notebook, we manually specified the function we were trying to minimize. For common loss functions, PyTorch comes with predefined classes. Our cross-entropy loss is for example implemented in this `BCELoss` class. You can explore other losses here: https://pytorch.org/docs/stable/nn.html.\n",
    "\n",
    "As to the optimizer, we use again the Stochastic Gradient Descent optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), # parameters to optimize\n",
    "                            lr=learning_rate,    # the speed in which we optimize them  / how fast the model learns (think step size) \n",
    "                            ) \n",
    "# optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Run the model for 100 epochs__\n",
    "Ok! Now we have some data, a model, a loss function, and a way to update the model to try minimize the loss function. Let's run 100 passes of gradient descent (on the full dataset, not on batches for now) and see where we get."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "sigmoid = nn.Sigmoid() # somet pytorch util to calculate the sigmoid function\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    y_predicted = sigmoid(model(X_train_vect))\n",
    "\n",
    "    # calucate loss / MSE\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    \n",
    "    # Backward pass / gradient and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # some print to see that it is running\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Check performance against test data__\n",
    "\n",
    "We use some utils from scikit-learn to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# these are the predictions of the model for the test set\n",
    "predicted = sigmoid(model(X_test_vect)).detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_score(y_test, predicted>0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, \n",
    "                            np.where(predicted > 0.5, 1, 0),\n",
    "                            target_names = [\"Negative\", \"Positive\"]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus task__:\n",
    "\n",
    "Our accuracy is better than chance, but not great. It may be due to many different reasons, such as our learning rate not being great. What happens if you change the learning rate? What happens if you use a different optimizer? Try uncomment the `Adam` optimizer defined in the optimizer cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deep(er) networks\n",
    "So far we have worked with a very simple model. But as we have seen in class, you can create arbitrarily complex models. Let's do that using PyTorch. We can use PyTorch Module class, which allows us to build arbitrarily complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_input_features=10):\n",
    "        super().__init__()\n",
    "        self.linear1 = nn.Linear(n_input_features, 30)\n",
    "        self.linear2 = nn.Linear(30, 30)\n",
    "        self.linear3 = nn.Linear(30, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.linear1(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear2(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        x = self.linear3(x)\n",
    "        y_pred = torch.sigmoid(x)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize model, loss, and optimizer\n",
    "model = Model(n_input_features=n_features)\n",
    "criterion = nn.BCELoss()\n",
    "optimizer = torch.optim.AdamW(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model) # this is how our model looks like -- with some sigmoid activations in between."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's train the model like we did before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 100\n",
    "loss_history = [] # here we will store the value of the loss at each epoch\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    y_predicted = model(X_train_vect)\n",
    "    loss = criterion(y_predicted, y_train)\n",
    "    loss_history.append(loss)\n",
    "    \n",
    "    # Backward pass / gradient and update\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    # zero grad before new step\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # some print to see that it is running\n",
    "    if (epoch + 1) % 10 == 0:\n",
    "        print(f\"epoch: {epoch+1}, loss = {loss.item():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_H = [val.item() for val in loss_history]\n",
    "fig, ax = plt.subplots()\n",
    "ax.plot(loss_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Task__\n",
    "- What happens to performance by training a more complex model? \n",
    "- How does perfomance change as the complexity of the model increases (e.g., if you add more layers, or use wider hidden layers)?\n",
    "- Do training and test performance change differently, as model complexity increases?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Bonus task__ [advanced]: \n",
    "\n",
    "Note that we have implemented classifiers using counts as inputs, but we could be doing the same thing using the average of Word2Vec vectors for all words in each target sentence. If you run out of things to do and your are very proficient in Python, you can try implement this approach!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp-e23",
   "language": "python",
   "name": "nlp-e23"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
