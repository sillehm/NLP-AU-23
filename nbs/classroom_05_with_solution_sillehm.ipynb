{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 5 - Training a Named Entity Recognition Model with a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classroom today focuses on using LSTMs to train a named entity recognition model, which is an example of a many-to-many recurrent model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A very short intro to NER\n",
    "Named entity recognition (NER) also known as named entity extraction, and entity identification is the task of tagging an entity is the task of extracting which seeks to extract named entities from unstructured text into predefined categories such as names, medical codes, quantities or similar.\n",
    "\n",
    "The most common variant is the [CoNLL-20003](https://www.clips.uantwerpen.be/conll2003/ner/) format which uses the categories, person (PER), organization (ORG) location (LOC) and miscellaneous (MISC), which for example denote cases such nationalies. For example:\n",
    "\n",
    "*Hello my name is $Roberta_{PER}$ I live in $Aarhus_{LOC}$ and work at $AU_{ORG}$.*\n",
    "\n",
    "For example, let's see how this works with ```spaCy```. NB: you might need to remember to install a ```spaCy``` model:\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "doc = nlp(\"Hello my name is Roberta. I live in Denmark and work at Aarhus University, I am Italian and today is Wednesday 25th.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging standards\n",
    "There exist different tag standards for NER. The most used one is the BIO-format which frames the task as token classification denoting inside, outside and beginning of a token. \n",
    "\n",
    "Words marked with *O* are not a named entity. Words with NER tags which start with *B-\\** indicate the start of a multiword entity (i.e. *B-ORG* for the *Aarhus* in *Aarhus University*), while *I-\\** indicate the continuation of a token (e.g. University).\n",
    "\n",
    "    B = Beginning\n",
    "    I = Inside\n",
    "    O = Outside\n",
    "\n",
    "<details>\n",
    "<summary>Q: What other formats and standards are available? What kinds of entities do they make it possible to tag?</summary>\n",
    "<br>\n",
    "You can see more examples on the spaCy documentation for their [different models(https://spacy.io/models/en)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for t in doc:\n",
    "    if t.ent_type:\n",
    "        print(t, f\"{t.ent_iob_}-{t.ent_type_}\")\n",
    "    else:\n",
    "        print(t, t.ent_iob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some challenges with NER\n",
    "While NER is currently framed as above this formulating does contain some limitations. \n",
    "\n",
    "For instance the entity Aarhus University really refers to both the location Aarhus, the University within Aarhus, thus nested NER (N-NER) argues that it would be more correct to tag it in a nested fashion as \\[\\[$Aarhus_{LOC}$\\] $University$\\]$_{ORG}$ (Plank, 2020). \n",
    "\n",
    "Other task also include named entity linking. Which is the task of linking an entity to e.g. a wikipedia entry, thus you have to both know that it is indeed an entity and which entity it is (if it is indeed a defined entity).\n",
    "\n",
    "We will be using Bi-LSTMs to train an NER model on a predifined data set which uses IOB tags of the kind we outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training in batches\n",
    "\n",
    "In previous classes, we discussed stochastic gradient descent on mini-batches as a way to achieve an optimal tradeoff between performance and stability.\n",
    "Let's implement batching!\n",
    "\n",
    "<details>\n",
    "<summary>Reminder: Why might it be a good idea to train on batches, rather than the whole dataset?</summary>\n",
    "<br>\n",
    "These batches are usually small (something like 32 instances at a time) but they have couple of important effects on training:\n",
    "\n",
    "- Batches can be processed in parallel, rather the sequentially. This can result in substantial speed up from computational perspective\n",
    "- Similarly, smaller batch sizes make it easier to fit training data into memory\n",
    "- Lastly,  smaller batch sizes are noisy, meaning that they have a regularizing effect and thus lead to less overfitting.\n",
    "\n",
    "In this notebook, we're going to be using batches of data to train our NER model. To do that, we first have to prepare our batches for training. You can read more about batching in [this blog post](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/).\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this allows us to look one step up in the directory\n",
    "# for importing custom modules from src\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.util import batch\n",
    "from src.LSTM import LSTMModel\n",
    "from src.embedding import gensim_to_torch_embedding\n",
    "\n",
    "import random\n",
    "\n",
    "# numpy and pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# types that will be useful once we define some functions\n",
    "from typing import Tuple\n",
    "\n",
    "# loading data and embeddings\n",
    "from datasets import load_dataset\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the datset using the ```load_dataset()``` function we've already seen. Here we take only the training data.\n",
    "\n",
    "When you've downloaded the dataset, you're welcome to save a local copy so that we don't need to constantly download it again everytime the code runs.\n",
    "\n",
    "Q: What do the ```train.features``` values refer to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DATASET\n",
    "dataset = load_dataset(\"conllpp\")\n",
    "train = dataset[\"train\"]\n",
    "\n",
    "# inspect the dataset\n",
    "train[\"tokens\"][:1]\n",
    "train[\"ner_tags\"][:1]\n",
    "\n",
    "# get number of classes\n",
    "num_classes = train.features[\"ner_tags\"].feature.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use ```gensim``` to get some pretrained word embeddings for the input layer to the model. \n",
    "\n",
    "In this example, we're going to use a GloVe model pretrained on Wikipedia, with 50 dimensions.\n",
    "\n",
    "I've provided a helper function to take the ```gensim``` embeddings and prepare them for ```pytorch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CONVERTING EMBEDDINGS\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# you can also try: model = gensim.models.KeyedVectors.load_word2vec_format(\"../../../819739/models/english/model.bin\", binary=True) -- note that dimensionality is different\n",
    "embedding_layer, vocab = gensim_to_torch_embedding(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a batch\n",
    "\n",
    "The first thing we want to do is to shuffle our dataset before training. \n",
    "\n",
    "Why might it be a good idea to shuffle the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "shuffled_train = dataset[\"train\"].shuffle(seed=1)\n",
    "validation = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to bundle the shuffled training data into smaller batches of predefined size. I've written a small utility function here to help. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Can you explain how the batch() function works?</summary>\n",
    "<br>\n",
    " Hint: Check out [this link](https://realpython.com/introduction-to-python-generators/).\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batches_tokens = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "batches_tags = batch(shuffled_train[\"ner_tags\"], batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use the ```tokens_to_idx()``` function below on our batches.\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is this function doing? Why is it doing it?</summary>\n",
    "<br>\n",
    "We're making everything lowercase and adding a new, arbitrary token called <UNK> to the vocabulary. This <UNK> means \"unknown\" and is used to replace out-of-vocabulary tokens in the data - i.e. tokens that don't appear in the vocabulary of the pretrained word embeddings.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(tokens, vocab=model.key_to_index):\n",
    "    \"\"\"\n",
    "    - Write documentation for this function including type hints for each argument and return statement\n",
    "    - What does the .get method do?\n",
    "    - Why lowercase?\n",
    "    \"\"\"\n",
    "    return [vocab.get(t.lower(), vocab[\"UNK\"]) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check below that everything is working as expected by testing it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using only the first batch\n",
    "batch_tokens = next(batches_tokens)\n",
    "batch_tags = next(batches_tags)\n",
    "batch_tok_idx = [tokens_to_idx(sent) for sent in batch_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with document classification, our model needs to take input sequences of a fixed length. To get around this we do a couple of different steps.\n",
    "\n",
    "- Find the length of the longest sequence in the batch\n",
    "- Pad shorter sequences to the max length using an arbitrary token like <PAD>\n",
    "- Give the <PAD> token a new label ```-1``` to differentiate it from the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute length of longest sentence in batch\n",
    "batch_max_len = max([len(s) for s in batch_tok_idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Can you figure out the logic of what is happening in the next two cells?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len))\n",
    "batch_labels = 9 * np.ones((batch_size, batch_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to the numpy array\n",
    "for i in range(batch_size):\n",
    "    tok_idx = batch_tok_idx[i]\n",
    "    tags = batch_tags[i]\n",
    "    size = len(tok_idx)\n",
    "\n",
    "    batch_input[i][:size] = tok_idx\n",
    "    batch_labels[i][:size] = tags"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to convert the arrays into ```pytorch``` tensors, ready for the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all data are indices, we convert them to torch LongTensors (integers)\n",
    "batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "    batch_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now batched and processed, we want to run it through our RNN the same way as when we trained a classifier.\n",
    "\n",
    "Q: Why is ```output_dim = num_classes + 1```?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = LSTMModel(\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes+1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "X = batch_input\n",
    "y = model(X)\n",
    "\n",
    "loss = model.loss_fn(outputs=y, labels=batch_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an LSTM with ```pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file [LSTM.py](../src/LSTM.py), I've aready created an LSTM for you using ```pytorch```. Take some time to read through the code and make sure you understand how it's built up.\n",
    "\n",
    "Some questions for you to discuss in groups:\n",
    "\n",
    "- How is an LSTM layer created using ```pytorch```? How does the code compare to the classifier code we used last week?\n",
    "- What's going on with that weird bit that says ```@staticmethod```?\n",
    "  - [This might help](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "- On the forward pass, we use ```log_softmax()``` to make output predictions. What is this, and how does it relate to the output from the sigmoid function that we used for classification?\n",
    "- How would we make this LSTM model *bidirectional* - i.e. make it a Bi-LSTM? \n",
    "  - Hint: Check the documentation for the LSTM layer on the ```pytorch``` website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the LSTM for named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last part of the notebook, we are going to use bits of code we have seen today (related to batching) and code from last week to set up training and evaluation for an LSTM doing named entity recognition. There are a few parts of the code you have to fill: work in groups and note that there is nothing new you need to do! All you are required to do has been done earlier in this notebook, or last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Package all the code from previous steps into a single function, which prepares a batch of data for the model (we will apply this to all batches in the following chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(tokens, labels, vocab) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Prepare a batch of data for training.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[List[str]]): A list of lists of tokens.\n",
    "        labels (List[List[int]]): A list of lists of labels.\n",
    "        vocab (dict): A dictionary defining the model's vocabulary\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple of tensors containing the tokens and labels.\n",
    "    \"\"\"\n",
    "    batch_size = len(tokens)\n",
    "\n",
    "    # TODO: convert tokens to vocabulary items using the tokens_to_idx function\n",
    "    # < Add code here >\n",
    "    batch_tok_idx = [tokens_to_idx(sent, vocab) for sent in tokens]\n",
    "    \n",
    "    # TODO: compute length of longest sentence in batch. Call your output batch_max_len\n",
    "    batch_max_len = max([len(s) for s in batch_tok_idx])\n",
    "\n",
    "    # TODO: Pad the data and flag padded vs non-padded with -1\n",
    "    batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len))\n",
    "    batch_labels = 9 * np.ones((batch_size, batch_max_len))\n",
    "\n",
    "    # TODO: copy the data to numpy array\n",
    "    for i in range(batch_size):\n",
    "        tok_idx = batch_tok_idx[i]\n",
    "        tags = labels[i]\n",
    "        size = len(tok_idx)\n",
    "        batch_input[i][:size] = tok_idx\n",
    "        batch_labels[i][:size] = tags\n",
    "\n",
    "    # TODO: convert data to tensors    \n",
    "    batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "        batch_labels\n",
    "    )\n",
    "\n",
    "    # TODO: return two outputs: batch_input, batch_labels, tensors containing respectively tokens and true labels\n",
    "    return batch_input, batch_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that trains the model. \n",
    "\n",
    "**Questions**:\n",
    "What is the last part of the function doing?\n",
    "Think back of our first neural networks class: which problem is this trying to prevent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs, training, validation, vocab, patience, batch_size):\n",
    "    \"\"\"\n",
    "    A function for training the model.\n",
    "    \"\"\"\n",
    "    best_val_loss = None\n",
    "\n",
    "    # TODO: apply the prepare_batch function to the validation set\n",
    "    X_val, y_val = prepare_batch(validation[\"tokens\"], validation[\"ner_tags\"], vocab)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches_processed = 0\n",
    "        print(f'*** Epoch {epoch+1} ***')\n",
    "        b_tokens = batch(training['tokens'], batch_size)\n",
    "        b_tags = batch(training['ner_tags'], batch_size)\n",
    "        for tokens, tags in zip(b_tokens, b_tags):\n",
    "            \n",
    "            batches_processed += 1\n",
    "            if batches_processed % 10 == 0:\n",
    "                print(f'Batch {batches_processed}')\n",
    "                \n",
    "            # prepare data\n",
    "            # TODO: apply the prepare_batch function to create inputs and outputs\n",
    "            X, y = prepare_batch(tokens, tags, vocab)\n",
    "\n",
    "            #TODO: perform a forward pass\n",
    "            y_hat = model(X)\n",
    "            #TODO: compute the loss using the loss_fn method from our model (take a look at src/LSTM.py)\n",
    "            #TODO: compute the gradients\n",
    "            #TODO: update the weights (hint: look at notebook from last week!)\n",
    "            loss = model.loss_fn(y_hat, y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        #  periodically calculate loss on validation set\n",
    "        if epoch % 5 == 0:\n",
    "            #TODO: perform a forward pass on the validation set\n",
    "            #TODO: compute the loss on the validation set (call it \"loss\")\n",
    "            y_hat = model(X_val)\n",
    "            loss = model.loss_fn(y_hat, y_val)\n",
    "\n",
    "            # QUESTION: what is this part of the code doing?\n",
    "            if best_val_loss is None or loss < best_val_loss:\n",
    "                best_val_loss = loss\n",
    "                torch.save(model, 'model.pt')\n",
    "                patience_ = patience\n",
    "            else:\n",
    "                patience_ -= 5\n",
    "                if patience_ <= 0:\n",
    "                    break\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that runs the whole thing (training and evaluation) end-to-end. Take some time to understand what this function does, and note down any questions you might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(gensim_embedding: str, batch_size: int, epochs: int, learning_rate: float, patience: int = 10, optimizer=0):\n",
    "    \"\"\"\n",
    "    A function that does end-to-end data prepraration, training, and evaluation\n",
    "    \"\"\"\n",
    "    # set a seed to make the results reproducible\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # use the function gensim_to_torch_embeddings to create embedding_layer and vocab\n",
    "    embeddings = api.load(gensim_embedding)\n",
    "    embedding_layer, vocab = gensim_to_torch_embedding(embeddings)\n",
    "\n",
    "    # Preparing data\n",
    "    # shuffle dataset\n",
    "    dataset = load_dataset(\"conllpp\")\n",
    "    train = dataset[\"train\"].shuffle(seed=1)\n",
    "    test = dataset[\"test\"]\n",
    "    validation = dataset[\"validation\"]\n",
    "\n",
    "    # Compute the number of classes for LSTM output (+1 for PAD)\n",
    "    num_classes = train.features[\"ner_tags\"].feature.num_classes\n",
    "\n",
    "    # Initialize the model\n",
    "    lstm = LSTMModel(num_classes+1, embedding_layer, 20)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer == 0:\n",
    "        optimizer = torch.optim.AdamW(lstm.parameters(), lr=learning_rate)\n",
    "    elif optimizer == 1:\n",
    "        optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train model with given settings\n",
    "    train_model(lstm, optimizer, epochs, train, validation, vocab, patience, batch_size)\n",
    "\n",
    "    # Load the best model\n",
    "    best = torch.load('model.pt')\n",
    "\n",
    "    # test it on test set\n",
    "    X, y = prepare_batch(test[\"tokens\"], test[\"ner_tags\"], vocab)\n",
    "    y_hat = best.predict(X)\n",
    "\n",
    "    # reformat results by removing pad tokens and flattening\n",
    "    y_hat_depadded = []\n",
    "    pos = 0\n",
    "    for sen in test[\"ner_tags\"]:\n",
    "        for i in range(pos, pos + len(sen)):\n",
    "            y_hat_depadded.append(y_hat[i])\n",
    "        pos += y.shape[1]\n",
    "\n",
    "    # flatten the test sentences into a single list\n",
    "    flat_tags = [item for sublist in test[\"ner_tags\"] for item in sublist]\n",
    "    \n",
    "    # get actual label\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    ner_dict = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC', 9:'NONE'}\n",
    "    actual.append([ner_dict.get(k, k) for k in flat_tags])\n",
    "    predicted.append([ner_dict.get(k, k) for k in y_hat_depadded])\n",
    "\n",
    "    # extract classification report\n",
    "    report = classification_report(actual, predicted)\n",
    "    print(report)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: use the run function to train and evaluate the model\n",
    "run(gensim_embedding='glove-wiki-gigaword-50', \n",
    "    batch_size=16, \n",
    "    epochs=10,\n",
    "    learning_rate=0.01, \n",
    "    patience=3, \n",
    "    optimizer=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "- How do results change between bidirectional and unidirectional models?\n",
    "- How does the size of the LSTM affect performance?\n",
    "- Is the performance of the model balanced for all classes?\n",
    "\n",
    "### Bonus task:\n",
    "- If you want to evaluate performance as a function of parameters systematically, try implement all this through scripts, and log results as separate files outputs\n",
    "- A good way to monitor training is by using Weights&Biases. Check out their documentation and feel free to experiment: https://docs.wandb.ai/guides/integrations/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
