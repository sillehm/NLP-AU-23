{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classroom 5 - Training a Named Entity Recognition Model with a LSTM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The classroom today focuses on using LSTMs to train a named entity recognition model, which is an example of a many-to-many recurrent model.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. A very short intro to NER\n",
    "Named entity recognition (NER) also known as named entity extraction, and entity identification is the task of tagging an entity is the task of extracting which seeks to extract named entities from unstructured text into predefined categories such as names, medical codes, quantities or similar.\n",
    "\n",
    "The most common variant is the [CoNLL-20003](https://www.clips.uantwerpen.be/conll2003/ner/) format which uses the categories, person (PER), organization (ORG) location (LOC) and miscellaneous (MISC), which for example denote cases such nationalies. For example:\n",
    "\n",
    "*Hello my name is $Roberta_{PER}$ I live in $Aarhus_{LOC}$ and work at $AU_{ORG}$.*\n",
    "\n",
    "For example, let's see how this works with ```spaCy```. NB: you might need to remember to install a ```spaCy``` model:\n",
    "\n",
    "```python -m spacy download en_core_web_sm```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# python -m spacy download en_core_web_sm\n",
    "import spacy\n",
    "\n",
    "nlp = spacy.load(\"en_core_web_md\")\n",
    "doc = nlp(\"Hello my name is Roberta. I live in Denmark and work at Aarhus University, I am Italian and today is Wednesday 25th.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">Hello my name is \n",
       "<mark class=\"entity\" style=\"background: #aa9cfc; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Roberta\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">PERSON</span>\n",
       "</mark>\n",
       ". I live in \n",
       "<mark class=\"entity\" style=\"background: #feca74; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Denmark\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">GPE</span>\n",
       "</mark>\n",
       " and work at \n",
       "<mark class=\"entity\" style=\"background: #7aecec; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Aarhus University\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">ORG</span>\n",
       "</mark>\n",
       ", I am \n",
       "<mark class=\"entity\" style=\"background: #c887fb; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Italian\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">NORP</span>\n",
       "</mark>\n",
       " and \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    today\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       " is \n",
       "<mark class=\"entity\" style=\"background: #bfe1d9; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
       "    Wednesday 25th\n",
       "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; vertical-align: middle; margin-left: 0.5rem\">DATE</span>\n",
       "</mark>\n",
       ".</div></span>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from spacy import displacy\n",
    "displacy.render(doc, style=\"ent\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tagging standards\n",
    "There exist different tag standards for NER. The most used one is the BIO-format which frames the task as token classification denoting inside, outside and beginning of a token. \n",
    "\n",
    "Words marked with *O* are not a named entity. Words with NER tags which start with *B-\\** indicate the start of a multiword entity (i.e. *B-ORG* for the *Aarhus* in *Aarhus University*), while *I-\\** indicate the continuation of a token (e.g. University).\n",
    "\n",
    "    B = Beginning\n",
    "    I = Inside\n",
    "    O = Outside\n",
    "\n",
    "<details>\n",
    "<summary>Q: What other formats and standards are available? What kinds of entities do they make it possible to tag?</summary>\n",
    "<br>\n",
    "You can see more examples on the spaCy documentation for their [different models(https://spacy.io/models/en)\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello O\n",
      "my O\n",
      "name O\n",
      "is O\n",
      "Roberta B-PERSON\n",
      ". O\n",
      "I O\n",
      "live O\n",
      "in O\n",
      "Denmark B-GPE\n",
      "and O\n",
      "work O\n",
      "at O\n",
      "Aarhus B-ORG\n",
      "University I-ORG\n",
      ", O\n",
      "I O\n",
      "am O\n",
      "Italian B-NORP\n",
      "and O\n",
      "today B-DATE\n",
      "is O\n",
      "Wednesday B-DATE\n",
      "25th I-DATE\n",
      ". O\n"
     ]
    }
   ],
   "source": [
    "for t in doc:\n",
    "    if t.ent_type:\n",
    "        print(t, f\"{t.ent_iob_}-{t.ent_type_}\")\n",
    "    else:\n",
    "        print(t, t.ent_iob_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some challenges with NER\n",
    "While NER is currently framed as above this formulating does contain some limitations. \n",
    "\n",
    "For instance the entity Aarhus University really refers to both the location Aarhus, the University within Aarhus, thus nested NER (N-NER) argues that it would be more correct to tag it in a nested fashion as \\[\\[$Aarhus_{LOC}$\\] $University$\\]$_{ORG}$ (Plank, 2020). \n",
    "\n",
    "Other task also include named entity linking. Which is the task of linking an entity to e.g. a wikipedia entry, thus you have to both know that it is indeed an entity and which entity it is (if it is indeed a defined entity).\n",
    "\n",
    "We will be using Bi-LSTMs to train an NER model on a predifined data set which uses IOB tags of the kind we outlined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Training in batches\n",
    "\n",
    "In previous classes, we discussed stochastic gradient descent on mini-batches as a way to achieve an optimal tradeoff between performance and stability.\n",
    "Let's implement batching!\n",
    "\n",
    "<details>\n",
    "<summary>Reminder: Why might it be a good idea to train on batches, rather than the whole dataset?</summary>\n",
    "<br>\n",
    "These batches are usually small (something like 32 instances at a time) but they have couple of important effects on training:\n",
    "\n",
    "- Batches can be processed in parallel, rather the sequentially. This can result in substantial speed up from computational perspective\n",
    "- Similarly, smaller batch sizes make it easier to fit training data into memory\n",
    "- Lastly,  smaller batch sizes are noisy, meaning that they have a regularizing effect and thus lead to less overfitting.\n",
    "\n",
    "In this notebook, we're going to be using batches of data to train our NER model. To do that, we first have to prepare our batches for training. You can read more about batching in [this blog post](https://machinelearningmastery.com/how-to-control-the-speed-and-stability-of-training-neural-networks-with-gradient-descent-batch-size/).\n",
    "\n",
    "</details>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/work/NLP-AU-23/env/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# this allows us to look one step up in the directory\n",
    "# for importing custom modules from src\n",
    "import sys\n",
    "sys.path.append(\"..\")\n",
    "from src.util import batch\n",
    "from src.LSTM import LSTMModel\n",
    "from src.embedding import gensim_to_torch_embedding\n",
    "\n",
    "import random\n",
    "\n",
    "# numpy and pytorch\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "# types that will be useful once we define some functions\n",
    "from typing import Tuple\n",
    "\n",
    "# loading data and embeddings\n",
    "from datasets import load_dataset\n",
    "import gensim\n",
    "import gensim.downloader as api"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can download the datset using the ```load_dataset()``` function we've already seen. Here we take only the training data.\n",
    "\n",
    "When you've downloaded the dataset, you're welcome to save a local copy so that we don't need to constantly download it again everytime the code runs.\n",
    "\n",
    "Q: What do the ```train.features``` values refer to? \\\n",
    "A: Is a dictionary which is the input to the model - contains tokens, and different tags (e.g. ner-tags)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading builder script: 100%|██████████| 8.73k/8.73k [00:00<00:00, 8.20MB/s]\n",
      "Downloading metadata: 100%|██████████| 3.35k/3.35k [00:00<00:00, 5.80MB/s]\n",
      "Downloading readme: 100%|██████████| 7.70k/7.70k [00:00<00:00, 6.42MB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset conllpp/conllpp to /home/ucloud/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading data: 3.28MB [00:00, 46.2MB/s]                  ]\n",
      "Downloading data: 827kB [00:00, 21.3MB/s]                   1.03s/it]\n",
      "Downloading data: 749kB [00:00, 22.4MB/s]                   1.30it/s]\n",
      "Downloading data files: 100%|██████████| 3/3 [00:02<00:00,  1.28it/s]\n",
      "Extracting data files: 100%|██████████| 3/3 [00:00<00:00, 845.74it/s]\n",
      "                                                                                         \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset conllpp downloaded and prepared to /home/ucloud/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:00<00:00, 485.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# DATASET\n",
    "dataset = load_dataset(\"conllpp\")\n",
    "train = dataset[\"train\"]\n",
    "\n",
    "# inspect the dataset\n",
    "train[\"tokens\"][:1]\n",
    "train[\"ner_tags\"][:1]\n",
    "\n",
    "# get number of classes\n",
    "num_classes = train.features[\"ner_tags\"].feature.num_classes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then use ```gensim``` to get some pretrained word embeddings for the input layer to the model. \n",
    "\n",
    "In this example, we're going to use a GloVe model pretrained on Wikipedia, with 50 dimensions.\n",
    "\n",
    "I've provided a helper function to take the ```gensim``` embeddings and prepare them for ```pytorch```."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 66.0/66.0MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# CONVERTING EMBEDDINGS\n",
    "model = api.load(\"glove-wiki-gigaword-50\")\n",
    "# you can also try: model = gensim.models.KeyedVectors.load_word2vec_format(\"../../../819739/models/english/model.bin\", binary=True) -- note that dimensionality is different\n",
    "embedding_layer, vocab = gensim_to_torch_embedding(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "gensim_to_torch_embedding is a function in the source embedding.py script made by Roberta \\\n",
    "We also get a vocab variable from the function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing a batch\n",
    "\n",
    "The first thing we want to do is to shuffle our dataset before training. \n",
    "\n",
    "Why might it be a good idea to shuffle the data? \\\n",
    "A: To avoud biases related to how the data is ordered - no structure in the data that can introduce biases "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle dataset\n",
    "shuffled_train = dataset[\"train\"].shuffle(seed=1)\n",
    "validation = dataset[\"validation\"]\n",
    "test = dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to bundle the shuffled training data into smaller batches of predefined size. I've written a small utility function here to help. \n",
    "\n",
    "<details>\n",
    "<summary>Q: Can you explain how the batch() function works?</summary>\n",
    "<br>\n",
    " Hint: Check out [this link](https://realpython.com/introduction-to-python-generators/).\n",
    "</details>\n",
    "\n",
    "A: See util.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "batches_tokens = batch(shuffled_train[\"tokens\"], batch_size)\n",
    "batches_tags = batch(shuffled_train[\"ner_tags\"], batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object batch at 0x7f191de910e0>\n"
     ]
    }
   ],
   "source": [
    "print(batches_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we want to use the ```tokens_to_idx()``` function below on our batches.\n",
    "\n",
    "<details>\n",
    "<summary>Q: What is this function doing? Why is it doing it?</summary>\n",
    "<br>\n",
    "We're making everything lowercase and adding a new, arbitrary token called <UNK> to the vocabulary. This <UNK> means \"unknown\" and is used to replace out-of-vocabulary tokens in the data - i.e. tokens that don't appear in the vocabulary of the pretrained word embeddings.\n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokens_to_idx(tokens, vocab=model.key_to_index):\n",
    "    \"\"\"\n",
    "    - Write documentation for this function including type hints for each argument and return statement\n",
    "    - What does the .get method do?\n",
    "    - Why lowercase?\n",
    "    \"\"\"\n",
    "    return [vocab.get(t.lower(), vocab[\"UNK\"]) for t in tokens] # get method that can be used on dictionaries \n",
    "    # get the value that is associated to t (regardles of lowercase/uppercase) and also index any tolken that is not defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll check below that everything is working as expected by testing it on a single batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample using only the first batch\n",
    "batch_tokens = next(batches_tokens)\n",
    "batch_tags = next(batches_tags)\n",
    "batch_tok_idx = [tokens_to_idx(sent) for sent in batch_tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with document classification, our model needs to take input sequences of a fixed length. To get around this we do a couple of different steps.\n",
    "\n",
    "- Find the length of the longest sequence in the batch\n",
    "- Pad shorter sequences to the max length using an arbitrary token like <PAD>\n",
    "- Give the <PAD> token a new label ```-1``` to differentiate it from the other labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43\n"
     ]
    }
   ],
   "source": [
    "# compute length of longest sentence in batch\n",
    "batch_max_len = max([len(s) for s in batch_tok_idx])\n",
    "print(batch_max_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Can you figure out the logic of what is happening in the next two cells? \\\n",
    "A: 1) initialise a matrix that has 1s - and the shape that we want in the end \n",
    "2) iterate over our samples and input the right values "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len)) # \n",
    "batch_labels = 9 * np.ones((batch_size, batch_max_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400001\n"
     ]
    }
   ],
   "source": [
    "print(vocab[\"PAD\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    65,   1147, 269547,  ..., 400001, 400001, 400001],\n",
       "        [  2327,     45, 400001,  ..., 400001, 400001, 400001],\n",
       "        [  2417,   5661,     23,  ..., 400001, 400001, 400001],\n",
       "        ...,\n",
       "        [155542,    176, 121172,  ..., 400001, 400001, 400001],\n",
       "        [400000,   1112,     21,  ..., 400001, 400001, 400001],\n",
       "        [   614,   1518,   3068,  ..., 400001, 400001, 400001]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# copy the data to the numpy array\n",
    "for i in range(batch_size):\n",
    "    tok_idx = batch_tok_idx[i]\n",
    "    tags = batch_tags[i]\n",
    "    size = len(tok_idx)\n",
    "\n",
    "    batch_input[i][:size] = tok_idx\n",
    "    batch_labels[i][:size] = tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 5., 0., ..., 9., 9., 9.],\n",
       "       [0., 0., 9., ..., 9., 9., 9.],\n",
       "       [0., 0., 0., ..., 9., 9., 9.],\n",
       "       ...,\n",
       "       [3., 0., 3., ..., 9., 9., 9.],\n",
       "       [0., 0., 0., ..., 9., 9., 9.],\n",
       "       [3., 4., 0., ..., 9., 9., 9.]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The last step is to convert the arrays into ```pytorch``` tensors, ready for the NN model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# since all data are indices, we convert them to torch LongTensors (integers)\n",
    "batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "    batch_labels\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With our data now batched and processed, we want to run it through our RNN the same way as when we trained a classifier.\n",
    "\n",
    "Q: Why is ```output_dim = num_classes + 1```? \\\n",
    "A: ???"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create model\n",
    "model = LSTMModel( # class defined in LSTM.py\n",
    "    embedding_layer=embedding_layer, output_dim=num_classes+1, hidden_dim_size=256\n",
    ")\n",
    "\n",
    "# Forward pass\n",
    "X = batch_input \n",
    "y = model(X)\n",
    "\n",
    "loss = model.loss_fn(outputs=y, labels=batch_labels)\n",
    "\n",
    "# All this is a single step on a single batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Creating an LSTM with ```pytorch```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the file [LSTM.py](../src/LSTM.py), I've aready created an LSTM for you using ```pytorch```. Take some time to read through the code and make sure you understand how it's built up.\n",
    "\n",
    "Some questions for you to discuss in groups:\n",
    "\n",
    "- How is an LSTM layer created using ```pytorch```? How does the code compare to the classifier code we used last week?\n",
    "- What's going on with that weird bit that says ```@staticmethod```?\n",
    "  - [This might help](https://realpython.com/instance-class-and-static-methods-demystified/).\n",
    "- On the forward pass, we use ```log_softmax()``` to make output predictions. What is this, and how does it relate to the output from the sigmoid function that we used for classification?\n",
    "- How would we make this LSTM model *bidirectional* - i.e. make it a Bi-LSTM? \n",
    "  - Hint: Check the documentation for the LSTM layer on the ```pytorch``` website."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Training the LSTM for named entity recognition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this last part of the notebook, we are going to use bits of code we have seen today (related to batching) and code from last week to set up training and evaluation for an LSTM doing named entity recognition. There are a few parts of the code you have to fill: work in groups and note that there is nothing new you need to do! All you are required to do has been done earlier in this notebook, or last week."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Task 1**: Package all the code from previous steps into a single function, which prepares a batch of data for the model (we will apply this to all batches in the following chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_batch(tokens, labels, vocab) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"Prepare a batch of data for training.\n",
    "\n",
    "    Args:\n",
    "        tokens (List[List[str]]): A list of lists of tokens.\n",
    "        labels (List[List[int]]): A list of lists of labels.\n",
    "        vocab (dict): A dictionary defining the model's vocabulary\n",
    "\n",
    "    Returns:\n",
    "        Tuple[torch.Tensor, torch.Tensor]: A tuple of tensors containing the tokens and labels.\n",
    "    \"\"\"\n",
    "    batch_size = len(tokens)\n",
    "\n",
    "    # TODO: convert tokens to vocabulary items using the tokens_to_idx function\n",
    "    # < Add code here >\n",
    "    batch_tok_idx = [tokens_to_idx(sent, vocab) for sent in tokens]\n",
    "    \n",
    "    # TODO: compute length of longest sentence in batch. Call your output batch_max_len\n",
    "    batch_max_len = max([len(s) for s in batch_tok_idx])\n",
    "\n",
    "    # TODO: Pad the data and flag padded vs non-padded with -1\n",
    "    batch_input = vocab[\"PAD\"] * np.ones((batch_size, batch_max_len))\n",
    "    batch_labels = 9 * np.ones((batch_size, batch_max_len))\n",
    "\n",
    "    # TODO: copy the data to numpy array\n",
    "    for i in range(batch_size):\n",
    "        tok_idx = batch_tok_idx[i]\n",
    "        tags = labels[i]\n",
    "        size = len(tok_idx)\n",
    "        batch_input[i][:size] = tok_idx\n",
    "        batch_labels[i][:size] = tags\n",
    "\n",
    "    # TODO: convert data to tensors    \n",
    "    batch_input, batch_labels = torch.LongTensor(batch_input), torch.LongTensor(\n",
    "        batch_labels\n",
    "    )\n",
    "\n",
    "    # TODO: return two outputs: batch_input, batch_labels, tensors containing respectively tokens and true labels\n",
    "    return batch_input, batch_labels "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's create a function that trains the model. \n",
    "\n",
    "**Questions**:\n",
    "What is the last part of the function doing?\n",
    "Think back of our first neural networks class: which problem is this trying to prevent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, epochs, training, validation, vocab, patience, batch_size):\n",
    "    \"\"\"\n",
    "    A function for training the model.\n",
    "    \"\"\"\n",
    "    best_val_loss = None\n",
    "\n",
    "    # TODO: apply the prepare_batch function to the validation set\n",
    "    X_val, y_val = prepare_batch(validation[\"tokens\"], validation[\"ner_tags\"], vocab)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        batches_processed = 0\n",
    "        print(f'*** Epoch {epoch+1} ***')\n",
    "        b_tokens = batch(training['tokens'], batch_size) # tokens in groups of 32\n",
    "        b_tags = batch(training['ner_tags'], batch_size) # apply the pepare batch function\n",
    "        for tokens, tags in zip(b_tokens, b_tags):\n",
    "            \n",
    "            batches_processed += 1\n",
    "            if batches_processed % 10 == 0:\n",
    "                print(f'Batch {batches_processed}')\n",
    "                \n",
    "            # prepare data\n",
    "            # TODO: apply the prepare_batch function to create inputs and outputs\n",
    "            X, y = prepare_batch(tokens, tags, vocab)\n",
    "\n",
    "            #TODO: perform a forward pass\n",
    "            y_hat = model(X)\n",
    "            #TODO: compute the loss using the loss_fn method from our model (take a look at src/LSTM.py)\n",
    "            #TODO: compute the gradients\n",
    "            #TODO: update the weights (hint: look at notebook from last week!)\n",
    "            loss = model.loss_fn(y_hat, y)\n",
    "            loss.backward() \n",
    "            optimizer.step()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        #  periodically calculate loss on validation set\n",
    "        if epoch % 5 == 0:\n",
    "            #TODO: perform a forward pass on the validation set\n",
    "            #TODO: compute the loss on the validation set (call it \"loss\")\n",
    "            y_hat = model(X_val)\n",
    "            loss = model.loss_fn(y_hat, y_val)\n",
    "\n",
    "            # QUESTION: what is this part of the code doing?\n",
    "            if best_val_loss is None or loss < best_val_loss: \n",
    "                best_val_loss = loss\n",
    "                torch.save(model, 'model.pt')\n",
    "                patience_ = patience\n",
    "            else:\n",
    "                patience_ -= 5 # patience parameter - 5 works in most cases, depends on the model and the task\n",
    "                if patience_ <= 0:\n",
    "                    break\n",
    "                # every 5 epochs compare the performance with the performance 5 epochs ago \n",
    "                # if it didn't improve then stop training because it is not learning more\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we define a function that runs the whole thing (training and evaluation) end-to-end. Take some time to understand what this function does, and note down any questions you might have."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from seqeval.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run(gensim_embedding: str, batch_size: int, epochs: int, learning_rate: float, patience: int = 10, optimizer=0):\n",
    "    \"\"\"\n",
    "    A function that does end-to-end data prepraration, training, and evaluation\n",
    "    \"\"\"\n",
    "    # set a seed to make the results reproducible\n",
    "    torch.manual_seed(0)\n",
    "    np.random.seed(0)\n",
    "    random.seed(0)\n",
    "\n",
    "    # use the function gensim_to_torch_embeddings to create embedding_layer and vocab\n",
    "    embeddings = api.load(gensim_embedding)\n",
    "    embedding_layer, vocab = gensim_to_torch_embedding(embeddings)\n",
    "\n",
    "    # Preparing data\n",
    "    # shuffle dataset\n",
    "    dataset = load_dataset(\"conllpp\")\n",
    "    train = dataset[\"train\"].shuffle(seed=1)\n",
    "    test = dataset[\"test\"]\n",
    "    validation = dataset[\"validation\"]\n",
    "\n",
    "    # Compute the number of classes for LSTM output (+1 for PAD)\n",
    "    num_classes = train.features[\"ner_tags\"].feature.num_classes\n",
    "\n",
    "    # Initialize the model\n",
    "    lstm = LSTMModel(num_classes+1, embedding_layer, 20)\n",
    "\n",
    "    # Initialize optimizer\n",
    "    if optimizer == 0:\n",
    "        optimizer = torch.optim.AdamW(lstm.parameters(), lr=learning_rate)\n",
    "    elif optimizer == 1:\n",
    "        optimizer = torch.optim.Adam(lstm.parameters(), lr=learning_rate)\n",
    "    else:\n",
    "        optimizer = torch.optim.RMSprop(lstm.parameters(), lr=learning_rate)\n",
    "\n",
    "    # train model with given settings\n",
    "    train_model(lstm, optimizer, epochs, train, validation, vocab, patience, batch_size)\n",
    "\n",
    "    # Load the best model\n",
    "    best = torch.load('model.pt')\n",
    "\n",
    "    # test it on test set\n",
    "    X, y = prepare_batch(test[\"tokens\"], test[\"ner_tags\"], vocab)\n",
    "    y_hat = best.predict(X)\n",
    "\n",
    "    # reformat results by removing pad tokens and flattening\n",
    "    y_hat_depadded = []\n",
    "    pos = 0\n",
    "    for sen in test[\"ner_tags\"]:\n",
    "        for i in range(pos, pos + len(sen)):\n",
    "            y_hat_depadded.append(y_hat[i])\n",
    "        pos += y.shape[1]\n",
    "\n",
    "    # flatten the test sentences into a single list\n",
    "    flat_tags = [item for sublist in test[\"ner_tags\"] for item in sublist]\n",
    "    \n",
    "    # get actual label\n",
    "    actual = []\n",
    "    predicted = []\n",
    "    ner_dict = {0:'O', 1:'B-PER', 2:'I-PER', 3:'B-ORG', 4:'I-ORG', 5:'B-LOC', 6:'I-LOC', 7:'B-MISC', 8:'I-MISC', 9:'NONE'}\n",
    "    actual.append([ner_dict.get(k, k) for k in flat_tags])\n",
    "    predicted.append([ner_dict.get(k, k) for k in y_hat_depadded])\n",
    "\n",
    "    # extract classification report\n",
    "    report = classification_report(actual, predicted)\n",
    "    print(report)\n",
    "\n",
    "    return None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:datasets.builder:Found cached dataset conllpp (/home/ucloud/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2)\n",
      "100%|██████████| 3/3 [00:00<00:00, 461.84it/s]\n",
      "WARNING:datasets.arrow_dataset:Loading cached shuffled indices for dataset at /home/ucloud/.cache/huggingface/datasets/conllpp/conllpp/1.0.0/04f15f257dff3fe0fb36e049b73d51ecdf382698682f5e590b7fb13898206ba2/cache-bba38be1af2f532c.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "*** Epoch 1 ***\n",
      "Batch 10\n",
      "Batch 20\n",
      "Batch 30\n",
      "Batch 40\n",
      "Batch 50\n",
      "Batch 60\n",
      "Batch 70\n",
      "Batch 80\n",
      "Batch 90\n",
      "Batch 100\n",
      "Batch 110\n",
      "Batch 120\n",
      "Batch 130\n",
      "Batch 140\n",
      "Batch 150\n",
      "Batch 160\n",
      "Batch 170\n",
      "Batch 180\n",
      "Batch 190\n",
      "Batch 200\n",
      "Batch 210\n",
      "Batch 220\n",
      "Batch 230\n",
      "Batch 240\n",
      "Batch 250\n",
      "Batch 260\n",
      "Batch 270\n",
      "Batch 280\n",
      "Batch 290\n",
      "Batch 300\n",
      "Batch 310\n",
      "Batch 320\n",
      "Batch 330\n",
      "Batch 340\n",
      "Batch 350\n",
      "Batch 360\n",
      "Batch 370\n",
      "Batch 380\n",
      "Batch 390\n",
      "Batch 400\n",
      "Batch 410\n",
      "Batch 420\n",
      "Batch 430\n",
      "Batch 440\n",
      "Batch 450\n",
      "Batch 460\n",
      "Batch 470\n",
      "Batch 480\n",
      "Batch 490\n",
      "Batch 500\n",
      "Batch 510\n",
      "Batch 520\n",
      "Batch 530\n",
      "Batch 540\n",
      "Batch 550\n",
      "Batch 560\n",
      "Batch 570\n",
      "Batch 580\n",
      "Batch 590\n",
      "Batch 600\n",
      "Batch 610\n",
      "Batch 620\n",
      "Batch 630\n",
      "Batch 640\n",
      "Batch 650\n",
      "Batch 660\n",
      "Batch 670\n",
      "Batch 680\n",
      "Batch 690\n",
      "Batch 700\n",
      "Batch 710\n",
      "Batch 720\n",
      "Batch 730\n",
      "Batch 740\n",
      "Batch 750\n",
      "Batch 760\n",
      "Batch 770\n",
      "Batch 780\n",
      "Batch 790\n",
      "Batch 800\n",
      "Batch 810\n",
      "Batch 820\n",
      "Batch 830\n",
      "Batch 840\n",
      "Batch 850\n",
      "Batch 860\n",
      "Batch 870\n",
      "*** Epoch 2 ***\n",
      "Batch 10\n",
      "Batch 20\n",
      "Batch 30\n",
      "Batch 40\n",
      "Batch 50\n",
      "Batch 60\n",
      "Batch 70\n",
      "Batch 80\n",
      "Batch 90\n",
      "Batch 100\n",
      "Batch 110\n",
      "Batch 120\n",
      "Batch 130\n",
      "Batch 140\n",
      "Batch 150\n",
      "Batch 160\n",
      "Batch 170\n",
      "Batch 180\n",
      "Batch 190\n",
      "Batch 200\n",
      "Batch 210\n",
      "Batch 220\n",
      "Batch 230\n",
      "Batch 240\n",
      "Batch 250\n",
      "Batch 260\n",
      "Batch 270\n",
      "Batch 280\n",
      "Batch 290\n",
      "Batch 300\n",
      "Batch 310\n",
      "Batch 320\n",
      "Batch 330\n",
      "Batch 340\n",
      "Batch 350\n",
      "Batch 360\n",
      "Batch 370\n",
      "Batch 380\n",
      "Batch 390\n",
      "Batch 400\n",
      "Batch 410\n",
      "Batch 420\n",
      "Batch 430\n",
      "Batch 440\n",
      "Batch 450\n",
      "Batch 460\n",
      "Batch 470\n",
      "Batch 480\n",
      "Batch 490\n",
      "Batch 500\n",
      "Batch 510\n",
      "Batch 520\n",
      "Batch 530\n",
      "Batch 540\n",
      "Batch 550\n",
      "Batch 560\n",
      "Batch 570\n",
      "Batch 580\n",
      "Batch 590\n",
      "Batch 600\n",
      "Batch 610\n",
      "Batch 620\n",
      "Batch 630\n",
      "Batch 640\n",
      "Batch 650\n",
      "Batch 660\n",
      "Batch 670\n",
      "Batch 680\n",
      "Batch 690\n",
      "Batch 700\n",
      "Batch 710\n",
      "Batch 720\n",
      "Batch 730\n",
      "Batch 740\n",
      "Batch 750\n",
      "Batch 760\n",
      "Batch 770\n",
      "Batch 780\n",
      "Batch 790\n",
      "Batch 800\n",
      "Batch 810\n",
      "Batch 820\n",
      "Batch 830\n",
      "Batch 840\n",
      "Batch 850\n",
      "Batch 860\n",
      "Batch 870\n",
      "*** Epoch 3 ***\n",
      "Batch 10\n",
      "Batch 20\n",
      "Batch 30\n",
      "Batch 40\n",
      "Batch 50\n",
      "Batch 60\n",
      "Batch 70\n",
      "Batch 80\n",
      "Batch 90\n",
      "Batch 100\n",
      "Batch 110\n",
      "Batch 120\n",
      "Batch 130\n",
      "Batch 140\n",
      "Batch 150\n",
      "Batch 160\n",
      "Batch 170\n",
      "Batch 180\n",
      "Batch 190\n",
      "Batch 200\n",
      "Batch 210\n",
      "Batch 220\n",
      "Batch 230\n",
      "Batch 240\n",
      "Batch 250\n",
      "Batch 260\n",
      "Batch 270\n",
      "Batch 280\n",
      "Batch 290\n",
      "Batch 300\n",
      "Batch 310\n",
      "Batch 320\n",
      "Batch 330\n",
      "Batch 340\n",
      "Batch 350\n",
      "Batch 360\n",
      "Batch 370\n",
      "Batch 380\n",
      "Batch 390\n",
      "Batch 400\n",
      "Batch 410\n",
      "Batch 420\n",
      "Batch 430\n",
      "Batch 440\n",
      "Batch 450\n",
      "Batch 460\n",
      "Batch 470\n",
      "Batch 480\n",
      "Batch 490\n",
      "Batch 500\n",
      "Batch 510\n",
      "Batch 520\n",
      "Batch 530\n",
      "Batch 540\n",
      "Batch 550\n",
      "Batch 560\n",
      "Batch 570\n",
      "Batch 580\n",
      "Batch 590\n",
      "Batch 600\n",
      "Batch 610\n",
      "Batch 620\n",
      "Batch 630\n",
      "Batch 640\n",
      "Batch 650\n",
      "Batch 660\n",
      "Batch 670\n",
      "Batch 680\n",
      "Batch 690\n",
      "Batch 700\n",
      "Batch 710\n",
      "Batch 720\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39m# TODO: use the run function to train and evaluate the model\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m run(gensim_embedding\u001b[39m=\u001b[39;49m\u001b[39m'\u001b[39;49m\u001b[39mglove-wiki-gigaword-50\u001b[39;49m\u001b[39m'\u001b[39;49m, \n\u001b[1;32m      3\u001b[0m     batch_size\u001b[39m=\u001b[39;49m\u001b[39m16\u001b[39;49m, \n\u001b[1;32m      4\u001b[0m     epochs\u001b[39m=\u001b[39;49m\u001b[39m10\u001b[39;49m,\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[39m=\u001b[39;49m\u001b[39m0.01\u001b[39;49m, \n\u001b[1;32m      6\u001b[0m     patience\u001b[39m=\u001b[39;49m\u001b[39m3\u001b[39;49m, \n\u001b[1;32m      7\u001b[0m     optimizer\u001b[39m=\u001b[39;49m\u001b[39m0\u001b[39;49m)\n",
      "Cell \u001b[0;32mIn[27], line 36\u001b[0m, in \u001b[0;36mrun\u001b[0;34m(gensim_embedding, batch_size, epochs, learning_rate, patience, optimizer)\u001b[0m\n\u001b[1;32m     33\u001b[0m     optimizer \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39moptim\u001b[39m.\u001b[39mRMSprop(lstm\u001b[39m.\u001b[39mparameters(), lr\u001b[39m=\u001b[39mlearning_rate)\n\u001b[1;32m     35\u001b[0m \u001b[39m# train model with given settings\u001b[39;00m\n\u001b[0;32m---> 36\u001b[0m train_model(lstm, optimizer, epochs, train, validation, vocab, patience, batch_size)\n\u001b[1;32m     38\u001b[0m \u001b[39m# Load the best model\u001b[39;00m\n\u001b[1;32m     39\u001b[0m best \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mmodel.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[22], line 30\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, optimizer, epochs, training, validation, vocab, patience, batch_size)\u001b[0m\n\u001b[1;32m     26\u001b[0m y_hat \u001b[39m=\u001b[39m model(X)\n\u001b[1;32m     27\u001b[0m \u001b[39m#TODO: compute the loss using the loss_fn method from our model (take a look at src/LSTM.py)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m#TODO: compute the gradients\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m#TODO: update the weights (hint: look at notebook from last week!)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss \u001b[39m=\u001b[39m model\u001b[39m.\u001b[39;49mloss_fn(y_hat, y)\n\u001b[1;32m     31\u001b[0m loss\u001b[39m.\u001b[39mbackward() \n\u001b[1;32m     32\u001b[0m optimizer\u001b[39m.\u001b[39mstep()\n",
      "File \u001b[0;32m/work/NLP-AU-23/nbs/../src/LSTM.py:67\u001b[0m, in \u001b[0;36mloss_fn\u001b[0;34m(outputs, labels)\u001b[0m\n\u001b[1;32m     64\u001b[0m #the number of tokens is the sum of elements in mask\n\u001b[1;32m     65\u001b[0m num_tokens = int(torch.sum(mask))\n\u001b[0;32m---> 67\u001b[0m #pick the values corresponding to labels and multiply by mask\n\u001b[1;32m     68\u001b[0m outputs = outputs[range(outputs.shape[0]), labels]*mask\n\u001b[1;32m     70\u001b[0m #cross entropy loss for all non 'PAD' tokens\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TODO: use the run function to train and evaluate the model\n",
    "run(gensim_embedding='glove-wiki-gigaword-50', \n",
    "    batch_size=16, \n",
    "    epochs=10,\n",
    "    learning_rate=0.01, \n",
    "    patience=3, \n",
    "    optimizer=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Task:\n",
    "- How do results change between bidirectional and unidirectional models?\n",
    "- How does the size of the LSTM affect performance?\n",
    "- Is the performance of the model balanced for all classes?\n",
    "\n",
    "### Bonus task:\n",
    "- If you want to evaluate performance as a function of parameters systematically, try implement all this through scripts, and log results as separate files outputs\n",
    "- A good way to monitor training is by using Weights&Biases. Check out their documentation and feel free to experiment: https://docs.wandb.ai/guides/integrations/pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
