## Additional resources
Here is a list of extra readings (optional!) or videos that are relevant to topics covered in our lectures. 
This may be updated over the semester.

### Lecture 1
* History of NLP, Part 1: https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-1-ffbcb937ebce and Part 2: https://medium.com/@antoine.louis/a-brief-history-of-natural-language-processing-part-2-f5e575e8e37
* Sebastian Ruder, Modern evolution of NLP: https://www.ruder.io/a-review-of-the-recent-history-of-nlp/

### Lecture 2
* Pennington, J., Socher, R., & Manning, C. D. (2014). "Glove: Global vectors for word representation". In Proceedings of the 2014 conference on empirical methods in natural language processing (EMNLP) (pp. 1532-1543).
* Rohde, D. L., Gonnerman, L. M., & Plaut, D. C. (2006). "An improved model of semantic similarity based on lexical co-occurrence". Communications of the ACM, 8(627-633), 116.
* Schnabel, T., Labutov, I., Mimno, D., & Joachims, T. (2015, September). Evaluation methods for unsupervised word embeddings. In Proceedings of the 2015 conference on empirical methods in natural language processing (pp. 298-307).
* Lecture 1 (https://www.youtube.com/watch?v=rmVRLeJRkl4&t=853s) and 2 (https://www.youtube.com/watch?v=gqaHkPEZAew&t=2984s) of Stanford’s CS224 course (note that this anticipates some of the topics for next week)
* For the more practically-minded, a short talk explaining the principles and state-of-the-art (2017) of distributional semantics: https://www.youtube.com/watch?v=hTmKoHJw3Mg 

### Lecture 3
* Mikolov, T., Sutskever, I., Chen, K., Corrado, G. S., & Dean, J. (2013). Distributed representations of words and phrases and their compositionality. Advances in neural information processing systems, 26.
* Bolukbasi, T., Chang, K. W., Zou, J. Y., Saligrama, V., & Kalai, A. T. (2016). Man is to computer programmer as woman is to homemaker? debiasing word embeddings. Advances in neural information processing systems, 29.
* If you want a deep dive into word embeddings, you can check out this playlist: https://www.youtube.com/playlist?list=PLhWB2ZsrULv-wEM8JDKA1zk8_2Lc88I-s

### Lecture 4
* Lecture 3 of Stanford’s CS224 course (https://youtu.be/X0Jw4kgaFlg?si=xQfneC22t8_xnxU-) - lots of maths, don't feel like you have to it all
* With same content; Jurafsky & Martin, 2020 * Chapter 7 (https://web.stanford.edu/~jurafsky/slp3/7.pdf) - also a bit dense
* You can also check out the first lecture in this MIT course: https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI - more "practical", and it concerns neural nets in general
* DeepLearning.AI course on Neural Networks (~Week 1-2): https://www.youtube.com/playlist?list=PLkDaE6sCZn6Ec-XTbcX1uRg2_u4xOEky0

### Lecture 5
* For an intro to RNN and LSTMs in the context of language modeling, check out lectures 5 and 6 of the Stanford CS224 course: https://www.youtube.com/playlist?list=PLoROMvodv4rOSH4v6133s9LFPRHjEmbmJ
* For RNNs and LSTMs in general, you can check out the second lecture in this MIT course:  https://www.youtube.com/playlist?list=PLtBw6njQRU-rwp5__7C0oIVt26ZgjG9NI

### Lecture 6
*  An excellent blog post on the transformer: https://jalammar.github.io/illustrated-transformer/ 

### Lecture 9
* There is a great video intro to RLHF on HuggingFace: https://youtu.be/2MBJOuVq380, or, alternatively, a blog post by the same author: https://huggingface.co/blog/rlhf

### Lecture 12
* Centre for Research on Foundation Models (CRFM), 2022: On the Opportunities and Risks of Foundation Models, https://arxiv.org/pdf/2108.07258.pdf
* Hovy, D., & Prabhumoye, S. (2021). "Five sources of bias in natural language processing". Language and Linguistics Compass, 15(8), e12432.